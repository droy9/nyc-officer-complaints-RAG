{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# NYC Police Accountability RAG System\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/droy9/nyc-officer-complaints-RAG/blob/main/complaintsrag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "A modular Retrieval-Augmented Generation system for querying NYC police misconduct data.\n",
        "\n",
        "## Architecture\n",
        "```\n",
        "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
        "│  Data Ingestors │ ──▶ │  Text Processor │ ──▶ │  Index Builder  │\n",
        "│  (Extensible)   │     │  (Narratives)   │     │  (FAISS)        │\n",
        "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
        "                                                        │\n",
        "┌─────────────────┐     ┌─────────────────┐             ▼\n",
        "│   LLM Gateway   │ ◀── │   RAG Pipeline  │ ◀── ┌─────────────────┐\n",
        "│   (Portkey)     │     │   (Orchestrator)│     │  Retriever      │\n",
        "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB_il5ojfrD_"
      },
      "outputs": [],
      "source": [
        "## 1. Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WknQJq7UivHk"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas faiss-cpu sentence-transformers portkey-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrGvrADqfrEJ"
      },
      "source": [
        "\"\"\"\n",
        "Configuration & Environment Setup\n",
        "==================================\n",
        "Centralized configuration for the RAG system. All paths, model names,\n",
        "and tunable parameters are defined here.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# LOGGING SETUP\n",
        "# ============================================================================\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(\"rag\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration for the RAG system.\"\"\"\n",
        "    \n",
        "    # Data paths\n",
        "    data_dir: str = \".\"\n",
        "    raw_data_filename: str = \"complaintclean.csv\"\n",
        "    processed_data_filename: str = \"complaintclean_narrative.csv\"\n",
        "    index_filename: str = \"complaints.faiss\"\n",
        "    metadata_filename: str = \"complaints_meta.json\"\n",
        "    \n",
        "    # Embedding model\n",
        "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embedding_dim: int = 384\n",
        "    \n",
        "    # Chunking parameters\n",
        "    chunk_size: int = 512  # Characters per chunk (was 200, too small)\n",
        "    chunk_overlap: int = 128  # Overlap between chunks\n",
        "    \n",
        "    # Retrieval parameters\n",
        "    retrieval_k: int = 4  # Number of chunks to retrieve\n",
        "    max_context_chars: int = 3000  # Max chars per retrieved chunk\n",
        "    \n",
        "    # LLM configuration\n",
        "    llm_model: str = \"@first-integrati-db9427/gemini-2.5-flash-lite\"\n",
        "    llm_max_tokens: int = 512\n",
        "    \n",
        "    # API keys (loaded from environment/Colab secrets)\n",
        "    portkey_api_key: Optional[str] = field(default=None, repr=False)\n",
        "    \n",
        "    @property\n",
        "    def raw_data_path(self) -> Path:\n",
        "        return Path(self.data_dir) / self.raw_data_filename\n",
        "    \n",
        "    @property\n",
        "    def processed_data_path(self) -> Path:\n",
        "        return Path(self.data_dir) / self.processed_data_filename\n",
        "    \n",
        "    @property\n",
        "    def index_path(self) -> Path:\n",
        "        return Path(self.data_dir) / self.index_filename\n",
        "    \n",
        "    @property\n",
        "    def metadata_path(self) -> Path:\n",
        "        return Path(self.data_dir) / self.metadata_filename\n",
        "    \n",
        "    def load_api_keys(self):\n",
        "        \"\"\"Load API keys from Colab secrets or environment variables.\"\"\"\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            self.portkey_api_key = userdata.get('PORT_KEY')\n",
        "            logger.info(\"Loaded API key from Colab secrets\")\n",
        "        except (ImportError, Exception):\n",
        "            self.portkey_api_key = os.getenv('PORTKEY_API_KEY')\n",
        "            if self.portkey_api_key:\n",
        "                logger.info(\"Loaded API key from environment variable\")\n",
        "        \n",
        "        if not self.portkey_api_key:\n",
        "            logger.warning(\"⚠️ No API key found. Set PORT_KEY in Colab secrets or PORTKEY_API_KEY env var.\")\n",
        "    \n",
        "    def validate(self):\n",
        "        \"\"\"Validate configuration and check required files exist.\"\"\"\n",
        "        errors = []\n",
        "        \n",
        "        if not self.raw_data_path.exists():\n",
        "            errors.append(f\"Raw data file not found: {self.raw_data_path}\")\n",
        "        \n",
        "        if errors:\n",
        "            for err in errors:\n",
        "                logger.error(err)\n",
        "            return False\n",
        "        \n",
        "        logger.info(\"✓ Configuration validated successfully\")\n",
        "        return True\n",
        "\n",
        "# Initialize global config\n",
        "config = Config()\n",
        "config.load_api_keys()\n",
        "\n",
        "print(f\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════╗\n",
        "║          NYC Police Accountability RAG System                ║\n",
        "╠══════════════════════════════════════════════════════════════╣\n",
        "║  Embedding Model: {config.embedding_model:<40} ║\n",
        "║  Chunk Size:      {config.chunk_size:<40} ║\n",
        "║  Retrieval K:     {config.retrieval_k:<40} ║\n",
        "║  LLM Model:       {config.llm_model[:40]:<40} ║\n",
        "║  API Key:         {'✓ Loaded' if config.portkey_api_key else '✗ Missing':<40} ║\n",
        "╚══════════════════════════════════════════════════════════════╝\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH2dpQgZWA7G"
      },
      "source": [
        "## 2. Data Ingestion Layer (Extensible)\n",
        "\n",
        "This layer uses an abstract base class pattern to support multiple data sources.\n",
        "To add a new data source:\n",
        "1. Create a new class inheriting from `DataIngestor`\n",
        "2. Implement `load()`, `validate()`, and `to_narrative()` methods\n",
        "3. Register it in the `INGESTORS` dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RdBPz3mwzA9K"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data Ingestion Layer\n",
        "====================\n",
        "Extensible framework for ingesting multiple data sources.\n",
        "Each data source has its own ingestor class that handles loading,\n",
        "validation, and narrative generation.\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================================\n",
        "# BASE INGESTOR CLASS\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class IngestedRecord:\n",
        "    \"\"\"Standardized record format across all data sources.\"\"\"\n",
        "    source_type: str  # e.g., \"ccrb_complaint\", \"sqf_encounter\", \"gang_database\"\n",
        "    record_id: str\n",
        "    narrative: str\n",
        "    metadata: Dict[str, Any]  # Structured fields for filtering\n",
        "\n",
        "\n",
        "class DataIngestor(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for data ingestors.\n",
        "    \n",
        "    To add a new data source:\n",
        "    1. Subclass DataIngestor\n",
        "    2. Implement load(), validate(), and to_narrative()\n",
        "    3. Register in INGESTORS dict at bottom of this cell\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.df: Optional[pd.DataFrame] = None\n",
        "        self.logger = logging.getLogger(f\"rag.ingestor.{self.source_name}\")\n",
        "    \n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def source_name(self) -> str:\n",
        "        \"\"\"Unique identifier for this data source.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def required_columns(self) -> List[str]:\n",
        "        \"\"\"List of columns that must exist in the source data.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def load(self, path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load data from the specified path.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def to_narrative(self, row: pd.Series) -> str:\n",
        "        \"\"\"Convert a single row to narrative text.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def validate(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validate that DataFrame has required structure.\"\"\"\n",
        "        missing = set(self.required_columns) - set(df.columns)\n",
        "        if missing:\n",
        "            self.logger.error(f\"Missing required columns: {missing}\")\n",
        "            return False\n",
        "        return True\n",
        "    \n",
        "    def safe_get(self, row: pd.Series, col: str, default: str = \"Unknown\") -> str:\n",
        "        \"\"\"Safely get a value from a row, handling missing/null values.\"\"\"\n",
        "        val = row.get(col)\n",
        "        if pd.isna(val) or val == \"Unlisted\" or val == \"\" or val is None:\n",
        "            return default\n",
        "        return str(val)\n",
        "    \n",
        "    def ingest(self, path: str, limit: Optional[int] = None) -> List[IngestedRecord]:\n",
        "        \"\"\"\n",
        "        Full ingestion pipeline: load → validate → transform.\n",
        "        \n",
        "        Args:\n",
        "            path: Path to the data file\n",
        "            limit: Optional limit on number of records (for testing)\n",
        "        \n",
        "        Returns:\n",
        "            List of IngestedRecord objects\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Loading data from {path}\")\n",
        "        \n",
        "        df = self.load(path)\n",
        "        \n",
        "        if limit:\n",
        "            self.logger.info(f\"Limiting to {limit} records (testing mode)\")\n",
        "            df = df.head(limit)\n",
        "        \n",
        "        if not self.validate(df):\n",
        "            raise ValueError(f\"Validation failed for {self.source_name}\")\n",
        "        \n",
        "        self.df = df\n",
        "        records = []\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                narrative = self.to_narrative(row)\n",
        "                record = IngestedRecord(\n",
        "                    source_type=self.source_name,\n",
        "                    record_id=str(row.get(self.id_column, idx)),\n",
        "                    narrative=narrative,\n",
        "                    metadata=self.extract_metadata(row)\n",
        "                )\n",
        "                records.append(record)\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Error processing row {idx}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        self.logger.info(f\"✓ Ingested {len(records)} records from {self.source_name}\")\n",
        "        return records\n",
        "    \n",
        "    @property\n",
        "    def id_column(self) -> str:\n",
        "        \"\"\"Column to use as record ID. Override if different.\"\"\"\n",
        "        return self.required_columns[0] if self.required_columns else \"index\"\n",
        "    \n",
        "    def extract_metadata(self, row: pd.Series) -> Dict[str, Any]:\n",
        "        \"\"\"Extract structured metadata for filtering. Override for custom fields.\"\"\"\n",
        "        return {}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CCRB COMPLAINTS INGESTOR\n",
        "# ============================================================================\n",
        "class CCRBComplaintIngestor(DataIngestor):\n",
        "    \"\"\"Ingestor for CCRB complaint data.\"\"\"\n",
        "    \n",
        "    @property\n",
        "    def source_name(self) -> str:\n",
        "        return \"ccrb_complaint\"\n",
        "    \n",
        "    @property\n",
        "    def required_columns(self) -> List[str]:\n",
        "        return [\n",
        "            \"complaint_id\", \"incident_date\", \"first_name\", \"last_name\",\n",
        "            \"location_type\", \"contact_reason\", \"fado_type\", \"allegation_cat\",\n",
        "            \"contact_outcome\", \"ccrb_disposition\", \"penalty_rec\", \"status_cat\",\n",
        "            \"officer_race\", \"officer_gender\", \"days_on_force\",\n",
        "            \"officer_incident_rank\", \"officer_current_rank\",\n",
        "            \"impacted_race\", \"impacted_gender\"\n",
        "        ]\n",
        "    \n",
        "    @property\n",
        "    def id_column(self) -> str:\n",
        "        return \"complaint_id\"\n",
        "    \n",
        "    def load(self, path: str) -> pd.DataFrame:\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    \n",
        "    def to_narrative(self, row: pd.Series) -> str:\n",
        "        \"\"\"Convert a CCRB complaint row to narrative text.\"\"\"\n",
        "        \n",
        "        # Incident overview\n",
        "        date = self.safe_get(row, 'incident_date', 'an unknown date')\n",
        "        date_str = f\"On {date}\" if date != 'an unknown date' else \"On an unknown date\"\n",
        "        \n",
        "        first_name = self.safe_get(row, 'first_name')\n",
        "        last_name = self.safe_get(row, 'last_name')\n",
        "        complaint_id = self.safe_get(row, 'complaint_id', 'Unknown ID')\n",
        "        \n",
        "        if first_name != \"Unknown\" and last_name != \"Unknown\":\n",
        "            officer_str = f\"complaint {complaint_id} was filed against Officer {first_name} {last_name}\"\n",
        "        else:\n",
        "            officer_str = f\"complaint {complaint_id} was filed against an unidentified officer\"\n",
        "        \n",
        "        location = self.safe_get(row, 'location_type', 'an unknown location')\n",
        "        location_str = f\"at a(n) {location}\" if location != 'an unknown location' else \"at an unknown location\"\n",
        "        \n",
        "        reason = self.safe_get(row, 'contact_reason', 'unspecified reason')\n",
        "        fado = self.safe_get(row, 'fado_type', 'Unknown')\n",
        "        allegation = self.safe_get(row, 'allegation_cat', 'Unknown')\n",
        "        outcome = self.safe_get(row, 'contact_outcome', 'Unknown')\n",
        "        disposition = self.safe_get(row, 'ccrb_disposition', 'Unknown')\n",
        "        penalty = self.safe_get(row, 'penalty_rec', 'Not Applicable')\n",
        "        status = self.safe_get(row, 'status_cat', 'Unknown')\n",
        "        \n",
        "        overview = (\n",
        "            f\"{date_str}, {officer_str} {location_str} for the reason: \\\"{reason}\\\". \"\n",
        "            f\"The allegation was classified as FADO type '{fado}' with specific allegation '{allegation}'. \"\n",
        "            f\"Contact outcome: {outcome}. CCRB disposition: \\\"{disposition}\\\". \"\n",
        "            f\"{'Penalty: ' + penalty + '.' if penalty != 'Not Applicable' else 'No penalty received.'} \"\n",
        "            f\"Status: {status}.\"\n",
        "        )\n",
        "        \n",
        "        # Officer demographics\n",
        "        o_race = self.safe_get(row, 'officer_race')\n",
        "        o_gender = self.safe_get(row, 'officer_gender')\n",
        "        o_days = self.safe_get(row, 'days_on_force')\n",
        "        o_inc_rank = self.safe_get(row, 'officer_incident_rank')\n",
        "        o_cur_rank = self.safe_get(row, 'officer_current_rank')\n",
        "        \n",
        "        officer_info = (\n",
        "            f\"Officer demographics: Race: {o_race}, Gender: {o_gender}. \"\n",
        "            f\"Days on force: {o_days}. \"\n",
        "            f\"Rank at incident: {o_inc_rank}, Current rank: {o_cur_rank}.\"\n",
        "        )\n",
        "        \n",
        "        # Impacted party demographics\n",
        "        i_race = self.safe_get(row, 'impacted_race')\n",
        "        i_gender = self.safe_get(row, 'impacted_gender')\n",
        "        \n",
        "        impacted_info = f\"Impacted party: Race: {i_race}, Gender: {i_gender}.\"\n",
        "        \n",
        "        return f\"{overview}\\n\\n{officer_info}\\n\\n{impacted_info}\"\n",
        "    \n",
        "    def extract_metadata(self, row: pd.Series) -> Dict[str, Any]:\n",
        "        \"\"\"Extract structured metadata for filtering.\"\"\"\n",
        "        return {\n",
        "            \"complaint_id\": self.safe_get(row, 'complaint_id'),\n",
        "            \"incident_date\": self.safe_get(row, 'incident_date'),\n",
        "            \"officer_name\": f\"{self.safe_get(row, 'first_name')} {self.safe_get(row, 'last_name')}\",\n",
        "            \"fado_type\": self.safe_get(row, 'fado_type'),\n",
        "            \"allegation_cat\": self.safe_get(row, 'allegation_cat'),\n",
        "            \"ccrb_disposition\": self.safe_get(row, 'ccrb_disposition'),\n",
        "            \"officer_race\": self.safe_get(row, 'officer_race'),\n",
        "            \"impacted_race\": self.safe_get(row, 'impacted_race'),\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STOP, QUESTION & FRISK INGESTOR (TEMPLATE FOR FUTURE)\n",
        "# ============================================================================\n",
        "class SQFIngestor(DataIngestor):\n",
        "    \"\"\"\n",
        "    Template ingestor for Stop, Question & Frisk data.\n",
        "    TODO: Implement when SQF data integration begins.\n",
        "    \"\"\"\n",
        "    \n",
        "    @property\n",
        "    def source_name(self) -> str:\n",
        "        return \"sqf_encounter\"\n",
        "    \n",
        "    @property\n",
        "    def required_columns(self) -> List[str]:\n",
        "        # Placeholder - update with actual SQF columns\n",
        "        return [\"stop_id\", \"date\", \"precinct\", \"suspect_race\", \"suspect_age\"]\n",
        "    \n",
        "    def load(self, path: str) -> pd.DataFrame:\n",
        "        raise NotImplementedError(\"SQF ingestor not yet implemented\")\n",
        "    \n",
        "    def to_narrative(self, row: pd.Series) -> str:\n",
        "        raise NotImplementedError(\"SQF ingestor not yet implemented\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# INGESTOR REGISTRY\n",
        "# ============================================================================\n",
        "INGESTORS: Dict[str, type] = {\n",
        "    \"ccrb\": CCRBComplaintIngestor,\n",
        "    \"sqf\": SQFIngestor,\n",
        "    # Add new ingestors here as they are implemented\n",
        "}\n",
        "\n",
        "def get_ingestor(source_type: str, config: Config) -> DataIngestor:\n",
        "    \"\"\"Factory function to get an ingestor by type.\"\"\"\n",
        "    if source_type not in INGESTORS:\n",
        "        raise ValueError(f\"Unknown ingestor type: {source_type}. Available: {list(INGESTORS.keys())}\")\n",
        "    return INGESTORS[source_type](config)\n",
        "\n",
        "# Quick test\n",
        "logger.info(f\"Available ingestors: {list(INGESTORS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0682553"
      },
      "outputs": [],
      "source": [
        "## 3. Text Processing & Chunking\n",
        "\n",
        "Handles text chunking with configurable size and overlap.\n",
        "The `TextProcessor` class provides utilities for preparing text for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLWlMZyy8fw_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Text Processing & Chunking\n",
        "===========================\n",
        "Utilities for preparing text for embedding.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Tuple\n",
        "import hashlib\n",
        "\n",
        "\n",
        "class TextProcessor:\n",
        "    \"\"\"\n",
        "    Handles text chunking and preprocessing for the RAG pipeline.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 128):\n",
        "        \"\"\"\n",
        "        Initialize the text processor.\n",
        "        \n",
        "        Args:\n",
        "            chunk_size: Maximum characters per chunk\n",
        "            chunk_overlap: Characters of overlap between consecutive chunks\n",
        "        \"\"\"\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.logger = logging.getLogger(\"rag.text_processor\")\n",
        "    \n",
        "    def chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into overlapping chunks.\n",
        "        \n",
        "        Args:\n",
        "            text: The text to chunk\n",
        "            \n",
        "        Returns:\n",
        "            List of text chunks\n",
        "        \"\"\"\n",
        "        text = str(text).strip()\n",
        "        \n",
        "        if len(text) <= self.chunk_size:\n",
        "            return [text] if text else []\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        \n",
        "        while start < len(text):\n",
        "            end = start + self.chunk_size\n",
        "            chunk = text[start:end]\n",
        "            \n",
        "            # Try to break at sentence boundary if possible\n",
        "            if end < len(text):\n",
        "                # Look for sentence-ending punctuation near the end\n",
        "                for punct in ['. ', '! ', '? ', '\\n']:\n",
        "                    last_punct = chunk.rfind(punct)\n",
        "                    if last_punct > self.chunk_size * 0.5:  # Only if in latter half\n",
        "                        chunk = chunk[:last_punct + 1]\n",
        "                        end = start + len(chunk)\n",
        "                        break\n",
        "            \n",
        "            chunks.append(chunk.strip())\n",
        "            \n",
        "            if end >= len(text):\n",
        "                break\n",
        "            \n",
        "            start = end - self.chunk_overlap\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def prepare_documents(\n",
        "        self, \n",
        "        records: List[IngestedRecord]\n",
        "    ) -> Tuple[List[str], List[dict]]:\n",
        "        \"\"\"\n",
        "        Prepare documents and metadata for indexing.\n",
        "        \n",
        "        Args:\n",
        "            records: List of IngestedRecord objects\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (documents, metadata) where each document is a chunk\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        metadata = []\n",
        "        \n",
        "        for record in records:\n",
        "            chunks = self.chunk_text(record.narrative)\n",
        "            \n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                # Prepend record ID to each chunk for context\n",
        "                doc = f\"[{record.source_type}:{record.record_id}]\\n\\n{chunk}\"\n",
        "                documents.append(doc)\n",
        "                \n",
        "                meta = {\n",
        "                    \"source_type\": record.source_type,\n",
        "                    \"record_id\": record.record_id,\n",
        "                    \"chunk_idx\": chunk_idx,\n",
        "                    \"total_chunks\": len(chunks),\n",
        "                    \"content\": doc,  # Store content for retrieval\n",
        "                    **record.metadata\n",
        "                }\n",
        "                metadata.append(meta)\n",
        "        \n",
        "        self.logger.info(f\"Prepared {len(documents)} chunks from {len(records)} records\")\n",
        "        return documents, metadata\n",
        "    \n",
        "    @staticmethod\n",
        "    def compute_hash(text: str) -> str:\n",
        "        \"\"\"Compute MD5 hash of text for cache keys.\"\"\"\n",
        "        return hashlib.md5(text.encode()).hexdigest()[:12]\n",
        "\n",
        "\n",
        "# Initialize with config\n",
        "text_processor = TextProcessor(\n",
        "    chunk_size=config.chunk_size,\n",
        "    chunk_overlap=config.chunk_overlap\n",
        ")\n",
        "\n",
        "# Quick test\n",
        "test_text = \"This is a test. \" * 50\n",
        "test_chunks = text_processor.chunk_text(test_text)\n",
        "logger.info(f\"Test chunking: {len(test_text)} chars → {len(test_chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PHvBFI7D5DAN"
      },
      "outputs": [],
      "source": [
        "## 4. Embedding & Index Building\n",
        "\n",
        "The `IndexBuilder` handles:\n",
        "- Embedding generation using sentence-transformers\n",
        "- FAISS index creation and persistence\n",
        "- Metadata storage for retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkZWHyd770bZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Embedding & Index Building\n",
        "===========================\n",
        "Handles embedding generation and FAISS index management.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Optional\n",
        "import time\n",
        "\n",
        "\n",
        "class IndexBuilder:\n",
        "    \"\"\"\n",
        "    Builds and manages the FAISS vector index.\n",
        "    \n",
        "    Supports:\n",
        "    - Batch embedding for memory efficiency\n",
        "    - Index persistence (save/load)\n",
        "    - Multiple index types (Flat, IVF for scalability)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"rag.index_builder\")\n",
        "        self.model: Optional[SentenceTransformer] = None\n",
        "        self.index: Optional[faiss.Index] = None\n",
        "        self.metadata: List[dict] = []\n",
        "    \n",
        "    def load_embedding_model(self) -> SentenceTransformer:\n",
        "        \"\"\"Load the embedding model (lazy loading).\"\"\"\n",
        "        if self.model is None:\n",
        "            self.logger.info(f\"Loading embedding model: {self.config.embedding_model}\")\n",
        "            self.model = SentenceTransformer(self.config.embedding_model)\n",
        "            self.logger.info(\"✓ Embedding model loaded\")\n",
        "        return self.model\n",
        "    \n",
        "    def embed_documents(\n",
        "        self, \n",
        "        documents: List[str], \n",
        "        batch_size: int = 256,\n",
        "        show_progress: bool = True\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Embed documents in batches for memory efficiency.\n",
        "        \n",
        "        Args:\n",
        "            documents: List of text documents to embed\n",
        "            batch_size: Number of documents to embed at once\n",
        "            show_progress: Whether to log progress\n",
        "            \n",
        "        Returns:\n",
        "            Numpy array of embeddings (normalized for cosine similarity)\n",
        "        \"\"\"\n",
        "        model = self.load_embedding_model()\n",
        "        \n",
        "        all_embeddings = []\n",
        "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i in range(0, len(documents), batch_size):\n",
        "            batch = documents[i:i + batch_size]\n",
        "            batch_num = i // batch_size + 1\n",
        "            \n",
        "            if show_progress and batch_num % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = i / elapsed if elapsed > 0 else 0\n",
        "                self.logger.info(\n",
        "                    f\"Embedding batch {batch_num}/{total_batches} \"\n",
        "                    f\"({rate:.0f} docs/sec)\"\n",
        "                )\n",
        "            \n",
        "            embeddings = model.encode(\n",
        "                batch,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True,  # Required for cosine similarity\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "            all_embeddings.append(embeddings)\n",
        "        \n",
        "        result = np.vstack(all_embeddings)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        self.logger.info(\n",
        "            f\"✓ Embedded {len(documents)} documents in {elapsed:.1f}s \"\n",
        "            f\"({len(documents)/elapsed:.0f} docs/sec)\"\n",
        "        )\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def build_index(\n",
        "        self, \n",
        "        documents: List[str], \n",
        "        metadata: List[dict],\n",
        "        use_ivf: bool = False,\n",
        "        nlist: int = 100\n",
        "    ) -> faiss.Index:\n",
        "        \"\"\"\n",
        "        Build a FAISS index from documents.\n",
        "        \n",
        "        Args:\n",
        "            documents: List of text documents\n",
        "            metadata: List of metadata dicts for each document\n",
        "            use_ivf: Whether to use IVF index (faster for large datasets)\n",
        "            nlist: Number of clusters for IVF index\n",
        "            \n",
        "        Returns:\n",
        "            FAISS index\n",
        "        \"\"\"\n",
        "        if len(documents) != len(metadata):\n",
        "            raise ValueError(\n",
        "                f\"Document count ({len(documents)}) != metadata count ({len(metadata)})\"\n",
        "            )\n",
        "        \n",
        "        # Generate embeddings\n",
        "        embeddings = self.embed_documents(documents)\n",
        "        dim = embeddings.shape[1]\n",
        "        \n",
        "        # Create index\n",
        "        if use_ivf and len(documents) > 10000:\n",
        "            # IVF index for large datasets (faster queries, needs training)\n",
        "            self.logger.info(f\"Building IVF index with {nlist} clusters\")\n",
        "            quantizer = faiss.IndexFlatIP(dim)\n",
        "            self.index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
        "            self.index.train(embeddings)\n",
        "            self.index.nprobe = min(10, nlist)  # Search 10 clusters\n",
        "        else:\n",
        "            # Flat index (exact search, good for <100k documents)\n",
        "            self.logger.info(\"Building Flat index (exact search)\")\n",
        "            self.index = faiss.IndexFlatIP(dim)\n",
        "        \n",
        "        # Add embeddings\n",
        "        self.index.add(embeddings.astype(np.float32))\n",
        "        self.metadata = metadata\n",
        "        \n",
        "        self.logger.info(f\"✓ Built index with {self.index.ntotal} vectors\")\n",
        "        \n",
        "        return self.index\n",
        "    \n",
        "    def save(self, index_path: str = None, metadata_path: str = None):\n",
        "        \"\"\"Save index and metadata to disk.\"\"\"\n",
        "        index_path = index_path or str(self.config.index_path)\n",
        "        metadata_path = metadata_path or str(self.config.metadata_path)\n",
        "        \n",
        "        if self.index is None:\n",
        "            raise ValueError(\"No index to save. Call build_index() first.\")\n",
        "        \n",
        "        faiss.write_index(self.index, index_path)\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(self.metadata, f)\n",
        "        \n",
        "        self.logger.info(f\"✓ Saved index to {index_path}\")\n",
        "        self.logger.info(f\"✓ Saved metadata to {metadata_path}\")\n",
        "    \n",
        "    def load(self, index_path: str = None, metadata_path: str = None) -> bool:\n",
        "        \"\"\"Load index and metadata from disk.\"\"\"\n",
        "        index_path = index_path or str(self.config.index_path)\n",
        "        metadata_path = metadata_path or str(self.config.metadata_path)\n",
        "        \n",
        "        try:\n",
        "            self.index = faiss.read_index(index_path)\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                self.metadata = json.load(f)\n",
        "            \n",
        "            self.logger.info(\n",
        "                f\"✓ Loaded index with {self.index.ntotal} vectors \"\n",
        "                f\"and {len(self.metadata)} metadata entries\"\n",
        "            )\n",
        "            return True\n",
        "            \n",
        "        except FileNotFoundError as e:\n",
        "            self.logger.warning(f\"Could not load index: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# Initialize builder\n",
        "index_builder = IndexBuilder(config)\n",
        "logger.info(\"IndexBuilder initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnexnep_woR",
        "outputId": "d97ca0c3-34fb-42a4-a9fc-31adfefe0fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bye!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Retrieval Engine\n",
        "=================\n",
        "Handles semantic search with caching, filtering, and metrics.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "from functools import lru_cache\n",
        "import time\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    \"\"\"A single retrieval result with score and metadata.\"\"\"\n",
        "    score: float\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"RetrievalResult(score={self.score:.3f}, record_id={self.metadata.get('record_id', 'N/A')})\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievalMetrics:\n",
        "    \"\"\"Metrics for a retrieval operation.\"\"\"\n",
        "    query: str\n",
        "    num_results: int\n",
        "    top_score: float\n",
        "    avg_score: float\n",
        "    retrieval_time_ms: float\n",
        "    timestamp: str = field(default_factory=lambda: time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "\n",
        "class Retriever:\n",
        "    \"\"\"\n",
        "    Semantic retrieval engine with caching and filtering support.\n",
        "    \n",
        "    Features:\n",
        "    - Cached query embeddings\n",
        "    - Metadata filtering\n",
        "    - Retrieval metrics\n",
        "    - Configurable scoring thresholds\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, index_builder: IndexBuilder, config: Config):\n",
        "        self.index_builder = index_builder\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"rag.retriever\")\n",
        "        self._query_cache: Dict[str, np.ndarray] = {}\n",
        "        self.metrics_history: List[RetrievalMetrics] = []\n",
        "    \n",
        "    def _embed_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Embed a query with caching.\"\"\"\n",
        "        cache_key = TextProcessor.compute_hash(query)\n",
        "        \n",
        "        if cache_key not in self._query_cache:\n",
        "            model = self.index_builder.load_embedding_model()\n",
        "            embedding = model.encode(\n",
        "                [query], \n",
        "                normalize_embeddings=True\n",
        "            ).astype(np.float32)\n",
        "            self._query_cache[cache_key] = embedding\n",
        "            \n",
        "            # Limit cache size\n",
        "            if len(self._query_cache) > 1000:\n",
        "                # Remove oldest entries (simple FIFO)\n",
        "                oldest_key = next(iter(self._query_cache))\n",
        "                del self._query_cache[oldest_key]\n",
        "        \n",
        "        return self._query_cache[cache_key]\n",
        "    \n",
        "    def retrieve(\n",
        "        self,\n",
        "        query: str,\n",
        "        k: int = None,\n",
        "        filters: Optional[Dict[str, Any]] = None,\n",
        "        min_score: float = 0.0\n",
        "    ) -> List[RetrievalResult]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents for a query.\n",
        "        \n",
        "        Args:\n",
        "            query: The search query\n",
        "            k: Number of results to return (default from config)\n",
        "            filters: Optional metadata filters (e.g., {\"fado_type\": \"Force\"})\n",
        "            min_score: Minimum similarity score threshold\n",
        "            \n",
        "        Returns:\n",
        "            List of RetrievalResult objects sorted by score\n",
        "        \"\"\"\n",
        "        k = k or self.config.retrieval_k\n",
        "        start_time = time.time()\n",
        "        \n",
        "        if self.index_builder.index is None:\n",
        "            raise ValueError(\"No index loaded. Call index_builder.load() or build_index() first.\")\n",
        "        \n",
        "        # Embed query\n",
        "        query_embedding = self._embed_query(query)\n",
        "        \n",
        "        # Search with over-retrieval if filtering\n",
        "        search_k = k * 3 if filters else k\n",
        "        distances, indices = self.index_builder.index.search(query_embedding, search_k)\n",
        "        \n",
        "        results = []\n",
        "        for idx, score in zip(indices[0], distances[0]):\n",
        "            if idx == -1:  # FAISS returns -1 for missing results\n",
        "                continue\n",
        "            \n",
        "            if score < min_score:\n",
        "                continue\n",
        "            \n",
        "            metadata = self.index_builder.metadata[idx]\n",
        "            \n",
        "            # Apply filters\n",
        "            if filters:\n",
        "                match = all(\n",
        "                    metadata.get(key) == value \n",
        "                    for key, value in filters.items()\n",
        "                )\n",
        "                if not match:\n",
        "                    continue\n",
        "            \n",
        "            # Get content (stored in metadata or reconstruct)\n",
        "            content = metadata.get(\"content\", f\"[Record: {metadata.get('record_id', idx)}]\")\n",
        "            \n",
        "            results.append(RetrievalResult(\n",
        "                score=float(score),\n",
        "                content=content[:self.config.max_context_chars],\n",
        "                metadata=metadata\n",
        "            ))\n",
        "            \n",
        "            if len(results) >= k:\n",
        "                break\n",
        "        \n",
        "        # Calculate metrics\n",
        "        elapsed_ms = (time.time() - start_time) * 1000\n",
        "        metrics = RetrievalMetrics(\n",
        "            query=query[:100],\n",
        "            num_results=len(results),\n",
        "            top_score=results[0].score if results else 0.0,\n",
        "            avg_score=sum(r.score for r in results) / len(results) if results else 0.0,\n",
        "            retrieval_time_ms=elapsed_ms\n",
        "        )\n",
        "        self.metrics_history.append(metrics)\n",
        "        \n",
        "        self.logger.info(\n",
        "            f\"Retrieved {len(results)} results in {elapsed_ms:.1f}ms \"\n",
        "            f\"(top score: {metrics.top_score:.3f})\"\n",
        "        )\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def get_context_block(\n",
        "        self, \n",
        "        results: List[RetrievalResult],\n",
        "        include_scores: bool = False\n",
        "    ) -> str:\n",
        "        \"\"\"Format retrieval results as a context block for the LLM.\"\"\"\n",
        "        blocks = []\n",
        "        for i, result in enumerate(results, 1):\n",
        "            header = f\"[{i}] Record: {result.metadata.get('record_id', 'Unknown')}\"\n",
        "            if include_scores:\n",
        "                header += f\" (relevance: {result.score:.2f})\"\n",
        "            blocks.append(f\"{header}\\n{result.content}\")\n",
        "        \n",
        "        return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "    \n",
        "    def get_citations(self, results: List[RetrievalResult]) -> str:\n",
        "        \"\"\"Generate citation string for results.\"\"\"\n",
        "        citations = []\n",
        "        for i, result in enumerate(results, 1):\n",
        "            meta = result.metadata\n",
        "            citations.append(\n",
        "                f\"[{i}] {meta.get('source_type', 'unknown')}:{meta.get('record_id', 'N/A')} \"\n",
        "                f\"(chunk {meta.get('chunk_idx', 0)+1}/{meta.get('total_chunks', 1)}, \"\n",
        "                f\"score: {result.score:.3f})\"\n",
        "            )\n",
        "        return \"\\n\".join(citations)\n",
        "    \n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clear the query embedding cache.\"\"\"\n",
        "        self._query_cache.clear()\n",
        "        self.logger.info(\"Query cache cleared\")\n",
        "\n",
        "\n",
        "# Initialize retriever (will be connected to index after loading)\n",
        "retriever = Retriever(index_builder, config)\n",
        "logger.info(\"Retriever initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LLM Integration\n",
        "\n",
        "Handles communication with the LLM via Portkey gateway.\n",
        "Includes error handling, retry logic, and response parsing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LLM Gateway\n",
        "============\n",
        "Handles all communication with the LLM API via Portkey.\n",
        "Includes error handling, retry logic, and rate limiting.\n",
        "\"\"\"\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMResponse:\n",
        "    \"\"\"Structured response from the LLM.\"\"\"\n",
        "    content: str\n",
        "    model: str\n",
        "    usage: Optional[Dict[str, int]] = None\n",
        "    latency_ms: float = 0.0\n",
        "    success: bool = True\n",
        "    error: Optional[str] = None\n",
        "\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Simple rate limiter for API calls.\"\"\"\n",
        "    \n",
        "    def __init__(self, max_calls: int = 10, period_seconds: int = 60):\n",
        "        self.max_calls = max_calls\n",
        "        self.period = period_seconds\n",
        "        self.calls: deque = deque()\n",
        "    \n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Block if rate limit would be exceeded.\"\"\"\n",
        "        now = time.time()\n",
        "        while self.calls and self.calls[0] < now - self.period:\n",
        "            self.calls.popleft()\n",
        "        if len(self.calls) >= self.max_calls:\n",
        "            sleep_time = self.calls[0] + self.period - now\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "        self.calls.append(time.time())\n",
        "\n",
        "\n",
        "class LLMGateway:\n",
        "    \"\"\"Gateway for LLM API calls via Portkey with retry and rate limiting.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"rag.llm_gateway\")\n",
        "        self.client = None\n",
        "        self.rate_limiter = RateLimiter(max_calls=10, period_seconds=60)\n",
        "        self._initialize_client()\n",
        "    \n",
        "    def _initialize_client(self):\n",
        "        if not self.config.portkey_api_key:\n",
        "            self.logger.warning(\"No API key configured. LLM calls will fail.\")\n",
        "            return\n",
        "        try:\n",
        "            from portkey_ai import Portkey\n",
        "            self.client = Portkey(api_key=self.config.portkey_api_key)\n",
        "            self.logger.info(\"✓ Portkey client initialized\")\n",
        "        except ImportError:\n",
        "            self.logger.error(\"portkey_ai not installed. Run: pip install portkey-ai\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize Portkey client: {e}\")\n",
        "    \n",
        "    def generate(self, system_prompt: str, user_prompt: str, max_tokens: int = None,\n",
        "                 temperature: float = 0.7, max_retries: int = 3) -> LLMResponse:\n",
        "        if self.client is None:\n",
        "            return LLMResponse(content=\"\", model=self.config.llm_model, success=False,\n",
        "                             error=\"LLM client not initialized. Check API key.\")\n",
        "        \n",
        "        max_tokens = max_tokens or self.config.llm_max_tokens\n",
        "        \n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                self.rate_limiter.wait_if_needed()\n",
        "                start_time = time.time()\n",
        "                \n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.config.llm_model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": user_prompt},\n",
        "                    ],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                )\n",
        "                \n",
        "                latency_ms = (time.time() - start_time) * 1000\n",
        "                \n",
        "                if not response.choices:\n",
        "                    raise ValueError(\"No choices in response\")\n",
        "                \n",
        "                content = response.choices[0].message.content or \"\"\n",
        "                usage = None\n",
        "                if hasattr(response, 'usage') and response.usage:\n",
        "                    usage = {\n",
        "                        \"prompt_tokens\": getattr(response.usage, 'prompt_tokens', 0),\n",
        "                        \"completion_tokens\": getattr(response.usage, 'completion_tokens', 0),\n",
        "                        \"total_tokens\": getattr(response.usage, 'total_tokens', 0),\n",
        "                    }\n",
        "                \n",
        "                self.logger.info(f\"LLM response: {latency_ms:.0f}ms ({len(content)} chars)\")\n",
        "                return LLMResponse(content=content, model=self.config.llm_model,\n",
        "                                 usage=usage, latency_ms=latency_ms, success=True)\n",
        "                \n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"LLM call failed (attempt {attempt + 1}/{max_retries}): {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                else:\n",
        "                    return LLMResponse(content=\"\", model=self.config.llm_model, success=False,\n",
        "                                     error=f\"LLM call failed after {max_retries} attempts: {e}\")\n",
        "        \n",
        "        return LLMResponse(content=\"\", model=self.config.llm_model, success=False,\n",
        "                         error=\"Unexpected error\")\n",
        "\n",
        "\n",
        "SYSTEM_PROMPTS = {\n",
        "    \"default\": \"\"\"You are a helpful assistant for the NYC Police Accountability Intelligence system.\n",
        "You help users understand police misconduct data from the CCRB database.\n",
        "\n",
        "When answering:\n",
        "1. Use ONLY the provided CONTEXT to answer\n",
        "2. Cite specific complaint IDs\n",
        "3. If context is insufficient, state what's missing\n",
        "4. Be precise, avoid speculation\n",
        "5. Remember: Allegations are not convictions. Note disposition status.\"\"\",\n",
        "\n",
        "    \"summary\": \"\"\"Summarize police misconduct patterns from the context.\n",
        "Focus on patterns, trends, and statistics. Cite complaint IDs.\"\"\",\n",
        "}\n",
        "\n",
        "llm_gateway = LLMGateway(config)\n",
        "logger.info(\"LLM Gateway initialized\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RAG PIPELINE ORCHESTRATOR\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class RAGResponse:\n",
        "    \"\"\"Complete response from the RAG pipeline.\"\"\"\n",
        "    answer: str\n",
        "    citations: str\n",
        "    retrieval_results: List[RetrievalResult]\n",
        "    llm_response: LLMResponse\n",
        "    success: bool = True\n",
        "    error: Optional[str] = None\n",
        "\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"Main orchestrator for the RAG system.\"\"\"\n",
        "    \n",
        "    def __init__(self, config, index_builder, retriever, llm_gateway, text_processor):\n",
        "        self.config = config\n",
        "        self.index_builder = index_builder\n",
        "        self.retriever = retriever\n",
        "        self.llm_gateway = llm_gateway\n",
        "        self.text_processor = text_processor\n",
        "        self.logger = logging.getLogger(\"rag.pipeline\")\n",
        "        self._is_initialized = False\n",
        "    \n",
        "    def build_index_from_source(self, source_type: str = \"ccrb\", data_path: str = None,\n",
        "                                 limit: Optional[int] = None, save: bool = True) -> int:\n",
        "        \"\"\"Build search index from data source.\"\"\"\n",
        "        data_path = data_path or str(self.config.raw_data_path)\n",
        "        ingestor = get_ingestor(source_type, self.config)\n",
        "        records = ingestor.ingest(data_path, limit=limit)\n",
        "        documents, metadata = self.text_processor.prepare_documents(records)\n",
        "        self.index_builder.build_index(documents, metadata)\n",
        "        if save:\n",
        "            self.index_builder.save()\n",
        "        self._is_initialized = True\n",
        "        self.logger.info(f\"✓ Pipeline initialized with {len(documents)} documents\")\n",
        "        return len(documents)\n",
        "    \n",
        "    def load_index(self) -> bool:\n",
        "        \"\"\"Load existing index from disk.\"\"\"\n",
        "        success = self.index_builder.load()\n",
        "        if success:\n",
        "            self._is_initialized = True\n",
        "            self.logger.info(\"✓ Pipeline loaded from existing index\")\n",
        "        return success\n",
        "    \n",
        "    def query(self, question: str, k: int = None, filters: Optional[Dict[str, Any]] = None,\n",
        "              system_prompt_key: str = \"default\") -> RAGResponse:\n",
        "        \"\"\"Process a user query through the full RAG pipeline.\"\"\"\n",
        "        if not self._is_initialized:\n",
        "            return RAGResponse(answer=\"\", citations=\"\", retrieval_results=[],\n",
        "                             llm_response=LLMResponse(content=\"\", model=\"\", success=False),\n",
        "                             success=False, error=\"Pipeline not initialized.\")\n",
        "        \n",
        "        self.logger.info(f\"Processing: {question[:50]}...\")\n",
        "        \n",
        "        try:\n",
        "            results = self.retriever.retrieve(question, k=k, filters=filters)\n",
        "        except Exception as e:\n",
        "            return RAGResponse(answer=\"\", citations=\"\", retrieval_results=[],\n",
        "                             llm_response=LLMResponse(content=\"\", model=\"\", success=False),\n",
        "                             success=False, error=f\"Retrieval failed: {e}\")\n",
        "        \n",
        "        if not results:\n",
        "            return RAGResponse(answer=\"No relevant information found.\", citations=\"\",\n",
        "                             retrieval_results=[], \n",
        "                             llm_response=LLMResponse(content=\"\", model=\"\", success=True),\n",
        "                             success=True)\n",
        "        \n",
        "        context_block = self.retriever.get_context_block(results)\n",
        "        citations = self.retriever.get_citations(results)\n",
        "        system_prompt = SYSTEM_PROMPTS.get(system_prompt_key, SYSTEM_PROMPTS[\"default\"])\n",
        "        \n",
        "        user_prompt = f\"QUESTION: {question}\\n\\nCONTEXT:\\n{context_block}\\n\\nProvide a comprehensive answer citing complaint IDs.\"\n",
        "        llm_response = self.llm_gateway.generate(system_prompt, user_prompt)\n",
        "        \n",
        "        return RAGResponse(answer=llm_response.content, citations=citations,\n",
        "                          retrieval_results=results, llm_response=llm_response,\n",
        "                          success=llm_response.success, error=llm_response.error)\n",
        "\n",
        "\n",
        "pipeline = RAGPipeline(config, index_builder, retriever, llm_gateway, text_processor)\n",
        "logger.info(\"RAG Pipeline initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Index (Run Once)\n",
        "\n",
        "Run this cell to ingest data and build the search index.\n",
        "Set `limit` to a small number for testing, or `None` for full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build index from CCRB complaint data\n",
        "# Set limit=100 for quick testing, limit=None for full dataset\n",
        "\n",
        "try:\n",
        "    # First try to load existing index\n",
        "    if pipeline.load_index():\n",
        "        print(\"✓ Loaded existing index\")\n",
        "    else:\n",
        "        # Build new index\n",
        "        print(\"Building new index from complaintclean.csv...\")\n",
        "        num_docs = pipeline.build_index_from_source(\n",
        "            source_type=\"ccrb\",\n",
        "            limit=None,  # Set to 100 for testing, None for full dataset\n",
        "            save=True\n",
        "        )\n",
        "        print(f\"✓ Built index with {num_docs} document chunks\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nMake sure 'complaintclean.csv' exists in the current directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Chat\n",
        "\n",
        "Query the RAG system with natural language questions about police misconduct data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(question: str, k: int = 4, filters: Dict = None) -> None:\n",
        "    \"\"\"\n",
        "    Convenience function to query the RAG system.\n",
        "    \n",
        "    Args:\n",
        "        question: Your question about police misconduct data\n",
        "        k: Number of documents to retrieve (default 4)\n",
        "        filters: Optional filters e.g. {\"fado_type\": \"Force\"}\n",
        "    \n",
        "    Examples:\n",
        "        ask(\"How many complaints involved excessive force?\")\n",
        "        ask(\"What are the most common allegation types?\")\n",
        "        ask(\"Show me complaints with substantiated outcomes\")\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    response = pipeline.query(question, k=k, filters=filters)\n",
        "    \n",
        "    if not response.success:\n",
        "        print(f\"\\n❌ Error: {response.error}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n📝 Answer:\\n{response.answer}\")\n",
        "    print(f\"\\n📚 Sources:\\n{response.citations}\")\n",
        "    if response.llm_response.latency_ms > 0:\n",
        "        print(f\"\\n⏱️ LLM Latency: {response.llm_response.latency_ms:.0f}ms\")\n",
        "\n",
        "\n",
        "print(\"RAG System Ready!\")\n",
        "print(\"\\nUse ask('your question') to query the system.\")\n",
        "print(\"\\nExample queries:\")\n",
        "print('  ask(\"How many complaints involved excessive force?\")')\n",
        "print('  ask(\"What are the most common allegation types?\")')\n",
        "print('  ask(\"Show me complaints against officers with multiple allegations\")')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example query - uncomment to test\n",
        "# ask(\"What types of complaints are most common in this dataset?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive chat loop - uncomment to start\n",
        "def chat():\n",
        "    \"\"\"Interactive chat loop for the RAG system.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NYC Police Accountability RAG Chat\")\n",
        "    print(\"Type 'quit' to exit, 'clear' to clear cache\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"\\nYou: \").strip()\n",
        "            \n",
        "            if not question:\n",
        "                continue\n",
        "            \n",
        "            if question.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\\nGoodbye!\")\n",
        "                break\n",
        "            \n",
        "            if question.lower() == 'clear':\n",
        "                retriever.clear_cache()\n",
        "                print(\"Cache cleared.\")\n",
        "                continue\n",
        "            \n",
        "            ask(question)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nGoodbye!\")\n",
        "            break\n",
        "\n",
        "# Uncomment to start interactive chat:\n",
        "# chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Adding New Data Sources\n",
        "\n",
        "To integrate a new data source (e.g., Stop-Question-Frisk, Gang Database):\n",
        "\n",
        "1. **Create an Ingestor Class** in Cell 5:\n",
        "```python\n",
        "class GangDatabaseIngestor(DataIngestor):\n",
        "    @property\n",
        "    def source_name(self) -> str:\n",
        "        return \"gang_database\"\n",
        "    \n",
        "    @property\n",
        "    def required_columns(self) -> List[str]:\n",
        "        return [\"entry_id\", \"date_added\", \"criteria\", ...]\n",
        "    \n",
        "    def load(self, path: str) -> pd.DataFrame:\n",
        "        return pd.read_csv(path)\n",
        "    \n",
        "    def to_narrative(self, row: pd.Series) -> str:\n",
        "        # Convert row to narrative text\n",
        "        return f\"On {row['date_added']}, ...\"\n",
        "```\n",
        "\n",
        "2. **Register in INGESTORS dict**:\n",
        "```python\n",
        "INGESTORS = {\n",
        "    \"ccrb\": CCRBComplaintIngestor,\n",
        "    \"sqf\": SQFIngestor,\n",
        "    \"gang_db\": GangDatabaseIngestor,  # Add here\n",
        "}\n",
        "```\n",
        "\n",
        "3. **Build Combined Index**:\n",
        "```python\n",
        "# Ingest multiple sources\n",
        "records = []\n",
        "for source in [\"ccrb\", \"sqf\", \"gang_db\"]:\n",
        "    ingestor = get_ingestor(source, config)\n",
        "    records.extend(ingestor.ingest(f\"{source}_data.csv\"))\n",
        "\n",
        "# Build unified index\n",
        "docs, meta = text_processor.prepare_documents(records)\n",
        "index_builder.build_index(docs, meta)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (video)",
      "language": "python",
      "name": "video"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "758b54b07f4628f2981a93ec2fa893cf8f1006e199b2e47f7b9610a507fb2008"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
