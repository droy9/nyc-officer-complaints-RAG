{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/droy9/nyc-officer-complaints-RAG/blob/main/complaintsrag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB_il5ojfrD_"
      },
      "outputs": [],
      "source": [
        "pip install portkey_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WknQJq7UivHk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrGvrADqfrEJ"
      },
      "source": [
        "# RAG with this data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH2dpQgZWA7G"
      },
      "source": [
        "Complaint Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RdBPz3mwzA9K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def narrative(r):\n",
        "    date = f\"On {r['incident_date']}\" if r['incident_date'] != \"Unlisted\" else \"On an unknown date\"\n",
        "    if r['first_name'] != \"Unlisted\" and r['last_name'] != \"Unlisted\":\n",
        "        on = f\", {r['complaint_id']} was filed against officer {r['first_name']}, {r['last_name']}\"\n",
        "    else:\n",
        "        on = \", against an unknown officer\"\n",
        "    where= f\" at a(n) {r['location_type']}.\" if r['location_type'] != \"Unlisted\" else \" at an unknown location.\"\n",
        "    reason =  f\" for the reason: \\\"{r['contact_reason']}\\\".\"\n",
        "    alleg = f\" The allegation was classified with the FADO type of {r['fado_type']} with a specific allegation of {r['allegation_cat']}.\"\n",
        "    outcome = f\" The contact outcome was {r['contact_outcome']}.\"\n",
        "    ccrb_disposition = f\"The ccrb_disposition is \\\"{r['ccrb_disposition']}\\\".\" if r['ccrb_disposition'] != \" Complainant Unavailable\" else \" The ccrb_disposition is unavailable.\"\n",
        "    penalty = f\" The penalty received is {r['penalty_rec']}.\" if r['penalty_rec'] != \"Not Applicable\" else \" There was no penalty received.\"\n",
        "    status_cat = f\" The complaint status is {r['status_cat']} as of 4/1/2021.\" if r['status_cat'] != \"Unlisted\" else \" The complaint status is unknown\"\n",
        "    overview = date + on + where + reason + alleg + outcome + ccrb_disposition + penalty + status_cat;\n",
        "\n",
        "    officer_race = f\"The officer race is {r['officer_race']}.\" if r['officer_race'] != \"Unlisted\" else \"The officer race is unknown.\"\n",
        "    officer_sex = f\" The officer sex is {r['officer_gender']}.\" if r['officer_gender'] != \"Unlisted\" else \" The officer sex is unknown.\"\n",
        "    days_on_force = f\" The officer was on force for {r['days_on_force']} days when this dataset was last recorded.\" if r['days_on_force'] != \"Unlisted\" else \" The officer was on force for an unknown number of days when this dataset was last recorded.\"\n",
        "    officer_incident_rank = f\" The officer rank at the time of the incident was {r['officer_incident_rank']}.\" if r['officer_incident_rank'] != \"Unlisted\" else \" The officer rank during the incident is unknown.\"\n",
        "    officer_current_rank = f\" The officer current rank is {r['officer_current_rank']}.\" if r['officer_current_rank'] != \"Unlisted\" else \" The officer current rank is unknown.\"\n",
        "    officer_statistics = officer_race + officer_sex + days_on_force + officer_incident_rank + officer_current_rank;\n",
        "\n",
        "    impacted_race = f\"The race of the victim / alleged victim is {r['impacted_race']}\" if r['impacted_race'] != \"Unlisted\" else \"The race of the victim / alleged victim is unknown.\"\n",
        "    impacted_gender = f\" The gender of the victim / alleged victim is {r['impacted_gender']}\" if r['impacted_gender'] != \"Unlisted\" else \" The gender of the victim / alleged victim is unknown.\"\n",
        "    impacted_statistics = impacted_race + impacted_gender;\n",
        "\n",
        "    summary = (\n",
        "    f\"{overview}\\n\\n\"\n",
        "    f\"{officer_statistics}\\n\\n\"\n",
        "    f\"{impacted_statistics}\"\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "df = pd.read_csv(\"complaintclean.csv\")\n",
        "df = df.head()\n",
        "summary = df.apply(narrative, axis=1)\n",
        "summary\n",
        "#print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0682553"
      },
      "outputs": [],
      "source": [
        "# Corrected code to assign the summary to a new column\n",
        "df['summary'] = summary\n",
        "\n",
        "# Display the first few rows with the new 'summary' column\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLWlMZyy8fw_"
      },
      "outputs": [],
      "source": [
        "# Read CSV files (adjust file paths to where your CSVs are saved)\n",
        "df.to_csv(\"complaintclean_narrative.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHvBFI7D5DAN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "##For RAG\n",
        "!pip install pandas faiss-cpu sentence-transformers portkey-ai streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkZWHyd770bZ"
      },
      "outputs": [],
      "source": [
        "#To Build Index\n",
        "import json\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "CSV_PATH = \"complaintclean_narrative.csv\" #change Path to where your CSV is\n",
        "META_PATH  = \"eebo_cc_meta.json\"\n",
        "INDEX_PATH = \"eebo_cc_allmini.cosine.faiss\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df = df[[\"complaint_id\", \"summary\"]].dropna(subset=[\"summary\"]).reset_index(drop=True)\n",
        "\n",
        "def chunk_text(t, size=800, overlap=100):\n",
        "    t = str(t)\n",
        "    if len(t) <= size:\n",
        "        return [t]\n",
        "    chunks, start = [], 0\n",
        "    while start < len(t):\n",
        "        end = start + size\n",
        "        chunks.append(t[start:end])\n",
        "        if end >= len(t): break\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "docs, meta = [], []\n",
        "for i, row in df.iterrows():\n",
        "    title = str((row[\"complaint_id\"] or \"\"))\n",
        "    text  = str(row[\"summary\"])\n",
        "    for j, ch in enumerate(chunk_text(text, size=200)):\n",
        "        docs.append((title + \"\\n\\n\" + ch).strip())\n",
        "        meta.append({\"row_id\": int(i), \"chunk_id\": int(j), \"title\": title})\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embs = model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "dim = embs.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)  # cosine if vectors are normalized\n",
        "index.add(embs)\n",
        "\n",
        "faiss.write_index(index, INDEX_PATH)\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f)\n",
        "\n",
        "print(f\"Built index with {len(docs)} chunks â†’ {INDEX_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kmnexnep_woR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97ca0c3-34fb-42a4-a9fc-31adfefe0fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bye!\n"
          ]
        }
      ],
      "source": [
        "#To chat using LLM API's from Portkey\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from portkey_ai import Portkey\n",
        "from google.colab import userdata\n",
        "\n",
        "#INDEX_PATH = \"eebo_1415.faiss\"\n",
        "INDEX_PATH = \"eebo_cc_allmini.cosine.faiss\"\n",
        "META_PATH  = \"eebo_cc_meta.json\"\n",
        "CSV_PATH   = \"complaintclean_narrative.csv\" #add in your PATH\n",
        "\n",
        "# --- Load retrieval artifacts ---\n",
        "index = faiss.read_index(INDEX_PATH)\n",
        "with open(META_PATH) as f:\n",
        "    META = json.load(f)\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)[[\"complaint_id\", \"summary\"]]\n",
        "\n",
        "# Embedder (must match build_index.py)\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def retrieve(query, k=4, max_ctx_chars=2000):\n",
        "    q = embedder.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    D, I = index.search(q, k)\n",
        "    hits = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        m = META[idx]\n",
        "        row = df.iloc[m[\"row_id\"]]\n",
        "        context = (str(row[\"complaint_id\"]) + \"\\n\\n\" + str(row[\"summary\"])).strip()\n",
        "        hits.append({\n",
        "            \"score\": float(score),\n",
        "            \"row_id\": m[\"row_id\"],\n",
        "            \"chunk_id\": m[\"chunk_id\"],\n",
        "            \"title\": m[\"title\"],\n",
        "            \"context\": context[:max_ctx_chars]\n",
        "        })\n",
        "    return hits\n",
        "\n",
        "# --- Portkey client (your format) ---\n",
        "# Make sure your Portkey API key is correctly set up in Colab's secrets with the name `PORT_KEY`\n",
        "# and has the required permissions.\n",
        "portkey = Portkey(\n",
        "  api_key = userdata.get('PORT_KEY')\n",
        ")\n",
        "\n",
        "MODEL = \"@first-integrati-db9427/gemini-2.5-flash-lite\"\n",
        "\n",
        "def call_llm_portkey(system_prompt, user_prompt):\n",
        "    response = portkey.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return response.choices[0].message.content # Access content attribute directly\n",
        "\n",
        "def answer(query):\n",
        "    contexts = retrieve(query, k=2)\n",
        "    ctx_block = \"\\n\\n---\\n\".join(\n",
        "        [f\"[{i+1}] Title: {c['title']}\\n{c['context']}\" for i, c in enumerate(contexts)]\n",
        "    )\n",
        "    cites = \"\\n\".join([f\"- [{i+1}] row_id={c['row_id']} chunk={c['chunk_id']} title={c['title']}\"\n",
        "                       for i, c in enumerate(contexts)])\n",
        "    system = (\n",
        "        \"You are a helpful assistant. Use the provided CONTEXT to answer.\\n\"\n",
        "        \"but if you need to, use verified external knowledge and make a disclaimer to let\"\n",
        "        \"the user know that you're using external data\"\n",
        "        \"Don't be afraid to summarize all of the data\"\n",
        "        \"If insufficient, say you don't know.\"\n",
        "    )\n",
        "    user = f\"QUESTION: {query}\\n\\nCONTEXT:\\n{ctx_block}\\n\\nReturn a concise answer\"\n",
        "    out = call_llm_portkey(system, user)\n",
        "    return out, cites\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"RAG chat ready (Portkey). Type a question (Ctrl+C to quit).\")\n",
        "    while True:\n",
        "        try:\n",
        "            q = input(\"\\nYou: \").strip()\n",
        "            if not q:\n",
        "                continue\n",
        "            ans, src = answer(q)\n",
        "            print(\"\\nAssistant:\", ans)\n",
        "            print(\"\\nSources:\\n\", src)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nBye!\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (video)",
      "language": "python",
      "name": "video"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "758b54b07f4628f2981a93ec2fa893cf8f1006e199b2e47f7b9610a507fb2008"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}